{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Running ALS on Outfit Recommendation (PySpark)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "System version: 3.9.23 | packaged by conda-forge | (main, Jun  4 2025, 18:02:02) \n",
                        "[Clang 18.1.8 ]\n",
                        "Spark version: 4.0.0\n"
                    ]
                }
            ],
            "source": [
                "import warnings\n",
                "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
                "\n",
                "import sys\n",
                "import pyspark\n",
                "from pyspark.ml.recommendation import ALS\n",
                "from pyspark.ml.feature import StringIndexer\n",
                "import pyspark.sql.functions as F\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.types import StructType, StructField\n",
                "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
                "\n",
                "from recommenders.utils.timer import Timer\n",
                "from recommenders.utils.notebook_utils import is_jupyter\n",
                "from recommenders.datasets.spark_splitters import spark_random_split\n",
                "from recommenders.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation\n",
                "from recommenders.utils.spark_utils import start_or_get_spark\n",
                "from recommenders.utils.notebook_utils import store_metadata\n",
                "\n",
                "# Dataset\n",
                "from datasets import outfits\n",
                "\n",
                "print(f\"System version: {sys.version}\")\n",
                "print(\"Spark version: {}\".format(pyspark.__version__))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "# top k items to recommend\n",
                "TOP_K = 1\n",
                "\n",
                "OUTFITS_DATA_SIZE = '100'\n",
                "\n",
                "# Column names for the dataset\n",
                "COL_USER = \"UserId\"\n",
                "COL_ITEM = \"Clothing\"\n",
                "COL_RATING = \"Rating\"\n",
                "COL_WEATHER = \"Weather\"\n",
                "COL_ITEM_ID = \"ClothingId\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING: Using incubator modules: jdk.incubator.vector\n",
                        "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
                        "25/10/05 10:37:41 WARN Utils: Your hostname, Khangs-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 10.24.77.42 instead (on interface en0)\n",
                        "25/10/05 10:37:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
                        "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
                        "Setting default log level to \"WARN\".\n",
                        "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
                        "25/10/05 10:37:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
                    ]
                }
            ],
            "source": [
                "# Start Spark session\n",
                "spark = start_or_get_spark(\"ALS PySpark\", memory=\"16g\")\n",
                "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Download Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+------+-------+-----------------+------+\n",
                        "|UserId|Weather|         Clothing|Rating|\n",
                        "+------+-------+-----------------+------+\n",
                        "|     1|  Humid|           Blazer|   2.2|\n",
                        "|     1|  Rainy|           Blazer|   2.8|\n",
                        "|     1|  Sunny|           Hoodie|   2.5|\n",
                        "|     1|  Sunny|            Jeans|   3.8|\n",
                        "|     2| Cloudy|           Hoodie|   4.1|\n",
                        "|     2| Cloudy|            Jeans|   3.8|\n",
                        "|     2|  Humid|         Cardigan|   4.0|\n",
                        "|     2|  Humid|          Joggers|   3.7|\n",
                        "|     2|  Rainy|Long-sleeve shirt|   3.8|\n",
                        "|     2|  Rainy|          T-shirt|   3.6|\n",
                        "|     2|  Snowy|           Hoodie|   3.7|\n",
                        "|     2|  Sunny|            Jeans|   3.5|\n",
                        "|     2|  Sunny|             Polo|   4.3|\n",
                        "|     2|  Windy|           Shorts|   3.0|\n",
                        "|     3|  Humid|           Chinos|   3.7|\n",
                        "|     3|  Rainy|Long-sleeve shirt|   3.2|\n",
                        "|     3|  Rainy|             Polo|   2.8|\n",
                        "|     3|  Snowy|             Coat|   4.8|\n",
                        "|     3|  Snowy|Long-sleeve shirt|   2.7|\n",
                        "|     3|  Sunny|           Shorts|   4.1|\n",
                        "+------+-------+-----------------+------+\n",
                        "only showing top 20 rows\n",
                        "Data after indexing the 'Clothing' column:\n",
                        "+------+-------+-----------------+------+----------+\n",
                        "|UserId|Weather|         Clothing|Rating|ClothingId|\n",
                        "+------+-------+-----------------+------+----------+\n",
                        "|     1|  Humid|           Blazer|   2.2|       3.0|\n",
                        "|     1|  Rainy|           Blazer|   2.8|       3.0|\n",
                        "|     1|  Sunny|           Hoodie|   2.5|       0.0|\n",
                        "|     1|  Sunny|            Jeans|   3.8|       5.0|\n",
                        "|     2| Cloudy|           Hoodie|   4.1|       0.0|\n",
                        "|     2| Cloudy|            Jeans|   3.8|       5.0|\n",
                        "|     2|  Humid|         Cardigan|   4.0|       9.0|\n",
                        "|     2|  Humid|          Joggers|   3.7|       7.0|\n",
                        "|     2|  Rainy|Long-sleeve shirt|   3.8|       2.0|\n",
                        "|     2|  Rainy|          T-shirt|   3.6|       8.0|\n",
                        "|     2|  Snowy|           Hoodie|   3.7|       0.0|\n",
                        "|     2|  Sunny|            Jeans|   3.5|       5.0|\n",
                        "|     2|  Sunny|             Polo|   4.3|      10.0|\n",
                        "|     2|  Windy|           Shorts|   3.0|      11.0|\n",
                        "|     3|  Humid|           Chinos|   3.7|       4.0|\n",
                        "|     3|  Rainy|Long-sleeve shirt|   3.2|       2.0|\n",
                        "|     3|  Rainy|             Polo|   2.8|      10.0|\n",
                        "|     3|  Snowy|             Coat|   4.8|       6.0|\n",
                        "|     3|  Snowy|Long-sleeve shirt|   2.7|       2.0|\n",
                        "|     3|  Sunny|           Shorts|   4.1|      11.0|\n",
                        "+------+-------+-----------------+------+----------+\n",
                        "only showing top 20 rows\n"
                    ]
                }
            ],
            "source": [
                "schema = StructType(\n",
                "    (\n",
                "        StructField(COL_USER, IntegerType()),\n",
                "        StructField(COL_WEATHER, StringType()),\n",
                "        StructField(COL_ITEM, StringType()),\n",
                "        StructField(COL_RATING, FloatType()),\n",
                "    )\n",
                ")\n",
                "\n",
                "data = outfits.load_spark_df(spark, size=None, schema=schema, filepath=\"./datasets/csv/example_feature1.csv\")\n",
                "data.show()\n",
                "\n",
                "indexer = StringIndexer(inputCol=COL_ITEM, outputCol=COL_ITEM_ID)\n",
                "indexed_data = indexer.fit(data).transform(data)\n",
                "print(\"Data after indexing the 'Clothing' column:\")\n",
                "indexed_data.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Splitting the Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "N train 77\n",
                        "N test 23\n"
                    ]
                }
            ],
            "source": [
                "train, test = spark_random_split(indexed_data, ratio=0.75, seed=123)\n",
                "print (\"N train\", train.cache().count())\n",
                "print (\"N test\", test.cache().count())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Training the Model and Getting Our Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "header = {\n",
                "    \"userCol\": COL_USER,\n",
                "    \"itemCol\": COL_ITEM_ID,\n",
                "    \"ratingCol\": COL_RATING,\n",
                "}\n",
                "\n",
                "\n",
                "als = ALS(\n",
                "    rank=10,\n",
                "    maxIter=15,\n",
                "    implicitPrefs=False,\n",
                "    regParam=0.05,\n",
                "    coldStartStrategy='drop',\n",
                "    nonnegative=False,\n",
                "    seed=42,\n",
                "    **header\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "25/10/05 10:38:00 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
                        "25/10/05 10:38:00 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Took 1.9142997500000014 seconds for training.\n"
                    ]
                }
            ],
            "source": [
                "with Timer() as train_time:\n",
                "    model = als.fit(train)\n",
                "\n",
                "print(\"Took {} seconds for training.\".format(train_time.interval))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 171:=======================================>             (149 + 8) / 200]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Took 1.5354195419999996 seconds for prediction.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "# with Timer() as test_time:\n",
                "\n",
                "#     # Get the cross join of all user-item pairs and score them.\n",
                "#     users = train.select(COL_USER).distinct()\n",
                "#     items = train.select(COL_ITEM).distinct()\n",
                "#     user_item = users.crossJoin(items)\n",
                "#     dfs_pred = model.transform(user_item)\n",
                "\n",
                "#     # Remove seen items.\n",
                "#     dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
                "#         train.alias(\"train\"),\n",
                "#         (dfs_pred[COL_USER] == train[COL_USER]) & (dfs_pred[COL_ITEM] == train[COL_ITEM]),\n",
                "#         how='outer'\n",
                "#     )\n",
                "\n",
                "#     top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[f\"train.{COL_RATING}\"].isNull()) \\\n",
                "#         .select('pred.' + COL_USER, 'pred.' + COL_ITEM, 'pred.' + \"prediction\")\n",
                "\n",
                "#     # In Spark, transformations are lazy evaluation\n",
                "#     # Use an action to force execute and measure the test time \n",
                "#     top_all.cache().count()\n",
                "\n",
                "# print(\"Took {} seconds for prediction.\".format(test_time.interval))\n",
                "\n",
                "with Timer() as test_time:\n",
                "    users = train.select(COL_USER).distinct()\n",
                "\n",
                "    items = train.select(COL_ITEM_ID).distinct()\n",
                "\n",
                "    user_item = users.crossJoin(items)\n",
                "    dfs_pred = model.transform(user_item)\n",
                "\n",
                "    top_all = dfs_pred.join(\n",
                "        indexed_data.select(COL_USER, COL_ITEM_ID),\n",
                "        on=[COL_USER, COL_ITEM_ID],\n",
                "        how='left_anti'\n",
                "    )\n",
                "\n",
                "    # Force execution to measure the time\n",
                "    top_all.cache().count()\n",
                "\n",
                "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+------+----------+----------+\n",
                        "|UserId|ClothingId|prediction|\n",
                        "+------+----------+----------+\n",
                        "|    12|         8| 0.8833362|\n",
                        "|     1|         8| 2.4692569|\n",
                        "|    13|         8| 1.5551647|\n",
                        "|    16|         8| 1.8367697|\n",
                        "|     3|         8|0.74171567|\n",
                        "|    20|         8| 1.3612454|\n",
                        "|    19|         8| 2.3161528|\n",
                        "|    15|         8| 1.9725528|\n",
                        "|     9|         8|  2.264041|\n",
                        "|     4|         8|  2.011328|\n",
                        "|     8|         8| 2.3636644|\n",
                        "|     7|         8| 2.2307923|\n",
                        "|    11|         8| 2.1407385|\n",
                        "|    18|         8| 2.1490989|\n",
                        "|    13|         0| 3.3109655|\n",
                        "|     6|         0| 2.5150957|\n",
                        "|     3|         0|  3.050418|\n",
                        "|     5|         0|  4.445485|\n",
                        "|    15|         0|  4.336204|\n",
                        "|    17|         0| 2.2741568|\n",
                        "+------+----------+----------+\n",
                        "only showing top 20 rows\n"
                    ]
                }
            ],
            "source": [
                "top_all.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "rank_eval = SparkRankingEvaluation(test, top_all, k = TOP_K, col_user=COL_USER, col_item=COL_ITEM_ID, \n",
                "                                    col_rating=COL_RATING, col_prediction=\"prediction\", \n",
                "                                    relevancy_method=\"top_k\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model:\tALS\n",
                        "Top K:\t1\n",
                        "MAP:\t0.000000\n",
                        "NDCG:\t0.000000\n",
                        "Precision@K:\t0.000000\n",
                        "Recall@K:\t0.000000\n"
                    ]
                }
            ],
            "source": [
                "print(\"Model:\\tALS\",\n",
                "      \"Top K:\\t%d\" % rank_eval.k,\n",
                "      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n",
                "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
                "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
                "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Evaluate Rating Prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+------+-------+-----------------+------+----------+----------+\n",
                        "|UserId|Weather|         Clothing|Rating|ClothingId|prediction|\n",
                        "+------+-------+-----------------+------+----------+----------+\n",
                        "|    12|  Snowy|          Joggers|   2.9|         7| 1.8935436|\n",
                        "|    12|  Windy|           Hoodie|   3.7|         0| 3.3782058|\n",
                        "|    12|  Windy|       Sweatshirt|   4.4|         1| 3.0695956|\n",
                        "|     1|  Sunny|           Hoodie|   2.5|         0| 3.9122157|\n",
                        "|     6|  Snowy|          T-shirt|   1.0|         8|  2.972186|\n",
                        "|    16|  Rainy|           Chinos|   3.8|         4|  2.062534|\n",
                        "|     3|  Snowy|Long-sleeve shirt|   2.7|         2| 3.2212994|\n",
                        "|     5|  Windy|          T-shirt|   3.7|         8| 2.2005248|\n",
                        "|    19|  Windy|           Shorts|   2.5|        11|  2.477874|\n",
                        "|     9|  Sunny|Long-sleeve shirt|   3.5|         2| 3.4481497|\n",
                        "|    17|  Snowy|       Sweatshirt|   5.0|         1| 2.0359073|\n",
                        "|     4|  Sunny|           Chinos|   3.5|         4| 1.7861462|\n",
                        "|     8|  Rainy|           Blazer|   2.2|         3| 2.9685452|\n",
                        "|     8|  Snowy|Long-sleeve shirt|   3.9|         2| 3.6110528|\n",
                        "|     7|  Humid|           Blazer|   2.6|         3| 3.2262785|\n",
                        "|     7|  Humid|         Cardigan|   3.7|         9| 3.3539186|\n",
                        "|     7|  Humid|       Sweatshirt|   1.8|         1|  3.752984|\n",
                        "|    11|  Sunny|          Joggers|   2.5|         7|  2.707053|\n",
                        "|     2|  Humid|         Cardigan|   4.0|         9|  3.025178|\n",
                        "|     2|  Sunny|             Polo|   4.3|        10|  3.003373|\n",
                        "+------+-------+-----------------+------+----------+----------+\n",
                        "only showing top 20 rows\n"
                    ]
                }
            ],
            "source": [
                "# Generate predicted ratings.\n",
                "prediction = model.transform(test)\n",
                "prediction.cache().show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model:\tALS rating prediction\n",
                        "RMSE:\t1.252579\n",
                        "MAE:\t1.019917\n",
                        "Explained variance:\t-0.762224\n",
                        "R squared:\t-0.906327\n"
                    ]
                }
            ],
            "source": [
                "rating_eval = SparkRatingEvaluation(test, prediction, col_user=COL_USER, col_item=COL_ITEM, \n",
                "                                    col_rating=COL_RATING, col_prediction=\"prediction\")\n",
                "\n",
                "print(\"Model:\\tALS rating prediction\",\n",
                "      \"RMSE:\\t%f\" % rating_eval.rmse(),\n",
                "      \"MAE:\\t%f\" % rating_eval.mae(),\n",
                "      \"Explained variance:\\t%f\" % rating_eval.exp_var(),\n",
                "      \"R squared:\\t%f\" % rating_eval.rsquared(), sep='\\n')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "# cleanup spark instance\n",
                "spark.stop()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "project-testing",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.23"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
