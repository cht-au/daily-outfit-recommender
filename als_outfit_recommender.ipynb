{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Running ALS on Outfit Recommendation (PySpark)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "System version: 3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]\n",
                        "Spark version: 4.0.0\n"
                    ]
                }
            ],
            "source": [
                "import warnings\n",
                "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
                "\n",
                "import sys\n",
                "import pyspark\n",
                "from pyspark.ml.recommendation import ALS\n",
                "from pyspark.ml.feature import StringIndexer\n",
                "import pyspark.sql.functions as F\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.types import StructType, StructField\n",
                "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
                "\n",
                "from recommenders.utils.timer import Timer\n",
                "from recommenders.utils.notebook_utils import is_jupyter\n",
                "from recommenders.datasets.spark_splitters import spark_random_split\n",
                "from recommenders.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation\n",
                "from recommenders.utils.spark_utils import start_or_get_spark\n",
                "from recommenders.utils.notebook_utils import store_metadata\n",
                "\n",
                "# Dataset\n",
                "from datasets import outfits\n",
                "\n",
                "print(f\"System version: {sys.version}\")\n",
                "print(\"Spark version: {}\".format(pyspark.__version__))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "# top k items to recommend\n",
                "TOP_K = 1\n",
                "\n",
                "OUTFITS_DATA_SIZE = '100'\n",
                "\n",
                "# Column names for the dataset\n",
                "COL_USER = \"UserId\"\n",
                "COL_ITEM = \"Clothing\"\n",
                "COL_RATING = \"Rating\"\n",
                "COL_WEATHER = \"Weather\"\n",
                "COL_ITEM_ID = \"ClothingId\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Start Spark session\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mstart_or_get_spark\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mALS PySpark\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m16g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.analyzer.failAmbiguousSelfJoin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[1;32mc:\\Users\\zivil\\Desktop\\ND\\cat-website\\daily-outfit-recommender\\recommenders\\utils\\spark_utils.py:73\u001b[0m, in \u001b[0;36mstart_or_get_spark\u001b[1;34m(app_name, url, memory, config, packages, jars, repositories)\u001b[0m\n\u001b[0;32m     70\u001b[0m spark_opts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.extraJavaOptions\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-Xss4m\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     72\u001b[0m spark_opts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetOrCreate()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark_opts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32m<string>:1\u001b[0m\n",
                        "File \u001b[1;32mc:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
                        "File \u001b[1;32mc:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\pyspark\\core\\context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 523\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
                        "File \u001b[1;32mc:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\pyspark\\core\\context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m     )\n\u001b[1;32m--> 205\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    208\u001b[0m         master,\n\u001b[0;32m    209\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    220\u001b[0m     )\n",
                        "File \u001b[1;32mc:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\pyspark\\core\\context.py:444\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 444\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
                        "File \u001b[1;32mc:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 108\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    112\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    113\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    114\u001b[0m     )\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# Start Spark session\n",
                "spark = start_or_get_spark(\"ALS PySpark\", memory=\"16g\")\n",
                "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Download Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+------+-------+-----------+------+\n",
                        "|UserId|Weather|   Clothing|Rating|\n",
                        "+------+-------+-----------+------+\n",
                        "|     1|  Sunny|    T-shirt|   5.0|\n",
                        "|     1|    Hot|     Shorts|   4.0|\n",
                        "|     1|  Rainy|     Hoodie|   3.0|\n",
                        "|     2|   Cold|Long Sleeve|   5.0|\n",
                        "|     2|  Windy|      Pants|   4.0|\n",
                        "|     2|  Sunny|     Jacket|   3.0|\n",
                        "|     3|    Hot|      Jeans|   5.0|\n",
                        "|     3|  Rainy|    Sweater|   4.0|\n",
                        "|     3|   Cold|       Polo|   3.0|\n",
                        "|     4|  Sunny|     Blazer|   5.0|\n",
                        "|     4|  Windy|      Skirt|   4.0|\n",
                        "|     4|  Rainy|   Cardigan|   3.0|\n",
                        "|     5|    Hot|     Hoodie|   5.0|\n",
                        "|     5|   Cold|    T-shirt|   4.0|\n",
                        "|     5|  Sunny|     Shorts|   3.0|\n",
                        "|     6|  Windy|    Sweater|   4.0|\n",
                        "|     6|  Sunny|     Jacket|   5.0|\n",
                        "|     6|   Cold|      Pants|   3.0|\n",
                        "|     7|    Hot|    T-shirt|   5.0|\n",
                        "|     7|  Rainy|     Hoodie|   4.0|\n",
                        "+------+-------+-----------+------+\n",
                        "only showing top 20 rows\n",
                        "Data after indexing the 'Clothing' column:\n",
                        "+------+-------+-----------+------+----------+\n",
                        "|UserId|Weather|   Clothing|Rating|ClothingId|\n",
                        "+------+-------+-----------+------+----------+\n",
                        "|     1|  Sunny|    T-shirt|   5.0|       1.0|\n",
                        "|     1|    Hot|     Shorts|   4.0|       2.0|\n",
                        "|     1|  Rainy|     Hoodie|   3.0|       0.0|\n",
                        "|     2|   Cold|Long Sleeve|   5.0|      10.0|\n",
                        "|     2|  Windy|      Pants|   4.0|       5.0|\n",
                        "|     2|  Sunny|     Jacket|   3.0|       3.0|\n",
                        "|     3|    Hot|      Jeans|   5.0|       7.0|\n",
                        "|     3|  Rainy|    Sweater|   4.0|       4.0|\n",
                        "|     3|   Cold|       Polo|   3.0|       8.0|\n",
                        "|     4|  Sunny|     Blazer|   5.0|       6.0|\n",
                        "|     4|  Windy|      Skirt|   4.0|      11.0|\n",
                        "|     4|  Rainy|   Cardigan|   3.0|       9.0|\n",
                        "|     5|    Hot|     Hoodie|   5.0|       0.0|\n",
                        "|     5|   Cold|    T-shirt|   4.0|       1.0|\n",
                        "|     5|  Sunny|     Shorts|   3.0|       2.0|\n",
                        "|     6|  Windy|    Sweater|   4.0|       4.0|\n",
                        "|     6|  Sunny|     Jacket|   5.0|       3.0|\n",
                        "|     6|   Cold|      Pants|   3.0|       5.0|\n",
                        "|     7|    Hot|    T-shirt|   5.0|       1.0|\n",
                        "|     7|  Rainy|     Hoodie|   4.0|       0.0|\n",
                        "+------+-------+-----------+------+----------+\n",
                        "only showing top 20 rows\n"
                    ]
                }
            ],
            "source": [
                "schema = StructType(\n",
                "    (\n",
                "        StructField(COL_USER, IntegerType()),\n",
                "        StructField(COL_WEATHER, StringType()),\n",
                "        StructField(COL_ITEM, StringType()),\n",
                "        StructField(COL_RATING, FloatType()),\n",
                "    )\n",
                ")\n",
                "\n",
                "data = outfits.load_spark_df(spark, size=None, schema=schema, filepath=\"./datasets/csv/own_feature.csv\")\n",
                "data.show()\n",
                "\n",
                "indexer = StringIndexer(inputCol=COL_ITEM, outputCol=COL_ITEM_ID)\n",
                "indexed_data = indexer.fit(data).transform(data)\n",
                "print(\"Data after indexing the 'Clothing' column:\")\n",
                "indexed_data.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Splitting the Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "N train 92\n",
                        "N test 28\n"
                    ]
                }
            ],
            "source": [
                "train, test = spark_random_split(indexed_data, ratio=0.75, seed=123)\n",
                "print (\"N train\", train.cache().count())\n",
                "print (\"N test\", test.cache().count())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Training the Model and Getting Our Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "header = {\n",
                "    \"userCol\": COL_USER,\n",
                "    \"itemCol\": COL_ITEM_ID,\n",
                "    \"ratingCol\": COL_RATING,\n",
                "}\n",
                "\n",
                "\n",
                "\n",
                "als = ALS(\n",
                "    rank=10,\n",
                "    maxIter=15,\n",
                "    implicitPrefs=False,\n",
                "    regParam=0.05,\n",
                "    coldStartStrategy='drop',\n",
                "    nonnegative=False,\n",
                "    seed=42,\n",
                "    **header\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Took 7.807712599998922 seconds for training.\n"
                    ]
                }
            ],
            "source": [
                "with Timer() as train_time:\n",
                "    model = als.fit(train)\n",
                "\n",
                "print(\"Took {} seconds for training.\".format(train_time.interval))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Took 4.616216800000984 seconds for prediction.\n"
                    ]
                }
            ],
            "source": [
                "# with Timer() as test_time:\n",
                "\n",
                "#     # Get the cross join of all user-item pairs and score them.\n",
                "#     users = train.select(COL_USER).distinct()\n",
                "#     items = train.select(COL_ITEM).distinct()\n",
                "#     user_item = users.crossJoin(items)\n",
                "#     dfs_pred = model.transform(user_item)\n",
                "\n",
                "#     # Remove seen items.\n",
                "#     dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
                "#         train.alias(\"train\"),\n",
                "#         (dfs_pred[COL_USER] == train[COL_USER]) & (dfs_pred[COL_ITEM] == train[COL_ITEM]),\n",
                "#         how='outer'\n",
                "#     )\n",
                "\n",
                "#     top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[f\"train.{COL_RATING}\"].isNull()) \\\n",
                "#         .select('pred.' + COL_USER, 'pred.' + COL_ITEM, 'pred.' + \"prediction\")\n",
                "\n",
                "#     # In Spark, transformations are lazy evaluation\n",
                "#     # Use an action to force execute and measure the test time \n",
                "#     top_all.cache().count()\n",
                "\n",
                "# print(\"Took {} seconds for prediction.\".format(test_time.interval))\n",
                "\n",
                "with Timer() as test_time:\n",
                "    users = train.select(COL_USER).distinct()\n",
                "\n",
                "    items = train.select(COL_ITEM_ID).distinct()\n",
                "\n",
                "    user_item = users.crossJoin(items)\n",
                "    dfs_pred = model.transform(user_item)\n",
                "\n",
                "    top_all = dfs_pred.join(\n",
                "        indexed_data.select(COL_USER, COL_ITEM_ID),\n",
                "        on=[COL_USER, COL_ITEM_ID],\n",
                "        how='left_anti'\n",
                "    )\n",
                "\n",
                "    # Force execution to measure the time\n",
                "    top_all.cache().count()\n",
                "\n",
                "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+------+----------+----------+\n",
                        "|UserId|ClothingId|prediction|\n",
                        "+------+----------+----------+\n",
                        "|     1|         7|  5.023626|\n",
                        "|     1|         8|   4.85732|\n",
                        "|     1|         6| 3.7927144|\n",
                        "|     1|         5| 3.5528836|\n",
                        "|     1|        11|  3.391508|\n",
                        "|     1|         9| 3.1464128|\n",
                        "|     1|        10| 2.9616942|\n",
                        "|     2|         7| 5.0620155|\n",
                        "|     2|         6|  4.709888|\n",
                        "|     2|        11| 4.1010056|\n",
                        "|     2|         4| 3.3333082|\n",
                        "|     2|         9| 3.1165466|\n",
                        "|     2|         8| 2.9009604|\n",
                        "|     3|         2|  4.277485|\n",
                        "|     3|         6| 4.1963844|\n",
                        "|     3|         5|  4.064257|\n",
                        "|     3|        10| 3.6585684|\n",
                        "|     3|         3|  3.625513|\n",
                        "|     3|        11| 3.5879092|\n",
                        "|     3|         9| 3.0859103|\n",
                        "+------+----------+----------+\n",
                        "only showing top 20 rows\n"
                    ]
                }
            ],
            "source": [
                "top_all.orderBy(COL_USER, F.desc(\"prediction\")).show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['UserId', 'Weather', 'Clothing', 'Rating', 'ClothingId']\n",
                        "['UserId', 'ClothingId', 'prediction']\n"
                    ]
                }
            ],
            "source": [
                "print(test.columns)\n",
                "print(top_all.columns)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rank_eval = SparkRankingEvaluation(test, top_all, k = TOP_K, col_user=COL_USER, col_item=COL_ITEM_ID, \n",
                "                                    col_rating=COL_RATING, col_prediction=\"prediction\", \n",
                "                                    relevancy_method=\"top_k\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model:\tALS\n",
                        "Top K:\t1\n",
                        "MAP:\t0.000000\n",
                        "NDCG:\t0.000000\n",
                        "Precision@K:\t0.000000\n",
                        "Recall@K:\t0.000000\n"
                    ]
                }
            ],
            "source": [
                "print(\"Model:\\tALS\",\n",
                "      \"Top K:\\t%d\" % rank_eval.k,\n",
                "      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n",
                "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
                "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
                "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Evaluate Rating Prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+------+-------+--------+------+----------+----------+\n",
                        "|UserId|Weather|Clothing|Rating|ClothingId|prediction|\n",
                        "+------+-------+--------+------+----------+----------+\n",
                        "|     1|    Hot|  Shorts|   5.0|         2| 3.9349766|\n",
                        "|    13|    Hot|    Polo|   5.0|         8|   4.20761|\n",
                        "|    13|  Sunny|   Pants|   5.0|         5| 3.3469734|\n",
                        "|     6|   Cold|   Pants|   3.0|         5| 4.2251115|\n",
                        "|     6|  Sunny| T-shirt|   4.0|         1| 4.5145974|\n",
                        "|     6|  Windy| Sweater|   4.0|         4| 4.7043977|\n",
                        "|     6|  Windy| Sweater|   4.0|         4| 4.7043977|\n",
                        "|    16|  Sunny|  Hoodie|   4.0|         0| 4.8954053|\n",
                        "|     3|   Cold|  Hoodie|   5.0|         0|  4.239459|\n",
                        "|     3|   Cold|    Polo|   3.0|         8| 3.8425083|\n",
                        "|    20|  Windy|  Blazer|   4.0|         6| 3.9430902|\n",
                        "|     5|  Windy|  Blazer|   5.0|         6|  4.475267|\n",
                        "|    19|  Windy|  Hoodie|   5.0|         0|  4.898927|\n",
                        "|    15|   Cold| T-shirt|   5.0|         1| 4.0074635|\n",
                        "|    17|  Windy|   Jeans|   4.0|         7|  4.431289|\n",
                        "|     4|    Hot|  Hoodie|   4.0|         0| 3.1139908|\n",
                        "|     4|  Windy|   Skirt|   4.0|        11| 3.1295557|\n",
                        "|     8|  Windy|  Shorts|   4.0|         2|  4.012461|\n",
                        "|     7|    Hot|  Hoodie|   5.0|         0| 4.0401506|\n",
                        "|     7|    Hot| T-shirt|   5.0|         1| 4.9066772|\n",
                        "+------+-------+--------+------+----------+----------+\n",
                        "only showing top 20 rows\n"
                    ]
                }
            ],
            "source": [
                "# Generate predicted ratings.\n",
                "prediction = model.transform(test)\n",
                "prediction.cache().show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model:\tALS rating prediction\n",
                        "RMSE:\t0.923172\n",
                        "MAE:\t0.756428\n",
                        "Explained variance:\t-0.725397\n",
                        "R squared:\t-0.727528\n"
                    ]
                }
            ],
            "source": [
                "rating_eval = SparkRatingEvaluation(test, prediction, col_user=COL_USER, col_item=COL_ITEM, \n",
                "                                    col_rating=COL_RATING, col_prediction=\"prediction\")\n",
                "\n",
                "print(\"Model:\\tALS rating prediction\",\n",
                "      \"RMSE:\\t%f\" % rating_eval.rmse(),\n",
                "      \"MAE:\\t%f\" % rating_eval.mae(),\n",
                "      \"Explained variance:\\t%f\" % rating_eval.exp_var(),\n",
                "      \"R squared:\\t%f\" % rating_eval.rsquared(), sep='\\n')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting iterative training (Rank=10, RegParam=0.05)...\n",
                        "Iteration 5: RMSE = 0.8682\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "ERROR:root:KeyboardInterrupt while sending command.\n",
                        "Traceback (most recent call last):\n",
                        "  File \"c:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
                        "    response = connection.send_command(command)\n",
                        "  File \"c:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\py4j\\clientserver.py\", line 535, in send_command\n",
                        "    answer = smart_decode(self.stream.readline()[:-1])\n",
                        "  File \"c:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\socket.py\", line 705, in readinto\n",
                        "    return self._sock.recv_into(b)\n",
                        "KeyboardInterrupt\n",
                        "ERROR:py4j.clientserver:Exception occurred while shutting down connection\n",
                        "Traceback (most recent call last):\n",
                        "  File \"c:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
                        "    response = connection.send_command(command)\n",
                        "  File \"c:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\py4j\\clientserver.py\", line 535, in send_command\n",
                        "    answer = smart_decode(self.stream.readline()[:-1])\n",
                        "  File \"c:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\socket.py\", line 705, in readinto\n",
                        "    return self._sock.recv_into(b)\n",
                        "KeyboardInterrupt\n",
                        "\n",
                        "During handling of the above exception, another exception occurred:\n",
                        "\n",
                        "Traceback (most recent call last):\n",
                        "  File \"c:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\py4j\\clientserver.py\", line 504, in shutdown_socket\n",
                        "    self.socket.sendall((\"%s\\n\" % address).encode(\"utf-8\"))\n",
                        "ConnectionAbortedError: [WinError 10053] An established connection was aborted by the software in your host machine\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[92], line 33\u001b[0m\n\u001b[0;32m     20\u001b[0m als_iter \u001b[38;5;241m=\u001b[39m ALS(\n\u001b[0;32m     21\u001b[0m     rank\u001b[38;5;241m=\u001b[39mFINAL_RANK,\n\u001b[0;32m     22\u001b[0m     maxIter\u001b[38;5;241m=\u001b[39mmax_iter, \u001b[38;5;66;03m# <--- The key difference: incrementing maxIter\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     ratingCol\u001b[38;5;241m=\u001b[39mCOL_RATING\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# b. Fit the model and generate predictions on the TEST set\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m model_iter \u001b[38;5;241m=\u001b[39m \u001b[43mals_iter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model_iter\u001b[38;5;241m.\u001b[39mtransform(test)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# c. Evaluate the model (calculate RMSE)\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\pyspark\\ml\\base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    208\u001b[0m     )\n",
                        "File \u001b[1;32mc:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\pyspark\\ml\\util.py:164\u001b[0m, in \u001b[0;36mtry_remote_fit.<locals>.wrapped\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\pyspark\\ml\\wrapper.py:411\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;129m@try_remote_fit\u001b[39m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 411\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
                        "File \u001b[1;32mc:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\pyspark\\ml\\wrapper.py:407\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\py4j\\java_gateway.py:1361\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
                        "File \u001b[1;32mc:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
                        "File \u001b[1;32mc:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\site-packages\\py4j\\clientserver.py:535\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    536\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[0;32m    537\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\zivil\\anaconda3\\envs\\outfit\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# --- 1. SETUP PARAMETERS AND VARIABLES ---\n",
                "# Assuming these variables are defined from your notebook's code:\n",
                "# COL_USER, COL_ITEM_ID, COL_RATING, train, test\n",
                "\n",
                "# Use the better-performing hyperparameters\n",
                "FINAL_RANK = 10 # 10 in original\n",
                "REG_PARAM = 0.05 # 0.05 in original\n",
                "MAX_ITER_RANGE = 25 # Number of training steps to observe\n",
                "\n",
                "error_data = []\n",
                "\n",
                "# --- 2. ITERATIVE TRAINING AND EVALUATION ---\n",
                "print(f\"Starting iterative training (Rank={FINAL_RANK}, RegParam={REG_PARAM})...\")\n",
                "\n",
                "for max_iter in range(1, MAX_ITER_RANGE + 1):\n",
                "    # a. Create a new ALS instance for the current iteration count\n",
                "    als_iter = ALS(\n",
                "        rank=FINAL_RANK,\n",
                "        maxIter=max_iter, # <--- The key difference: incrementing maxIter\n",
                "        implicitPrefs=False,\n",
                "        regParam=REG_PARAM,\n",
                "        coldStartStrategy='drop',\n",
                "        seed=42,\n",
                "        userCol=COL_USER,\n",
                "        itemCol=COL_ITEM_ID,\n",
                "        ratingCol=COL_RATING\n",
                "    )\n",
                "    \n",
                "    # b. Fit the model and generate predictions on the TEST set\n",
                "    model_iter = als_iter.fit(train)\n",
                "    prediction = model_iter.transform(test)\n",
                "    \n",
                "    # c. Evaluate the model (calculate RMSE)\n",
                "    rating_eval = SparkRatingEvaluation(\n",
                "        test, prediction, \n",
                "        col_user=COL_USER, \n",
                "        col_item=COL_ITEM_ID, \n",
                "        col_rating=COL_RATING, \n",
                "        col_prediction=\"prediction\"\n",
                "    )\n",
                "    \n",
                "    rmse_value = rating_eval.rmse()\n",
                "    error_data.append((max_iter, rmse_value))\n",
                "    \n",
                "    if max_iter % 5 == 0:\n",
                "        print(f\"Iteration {max_iter}: RMSE = {rmse_value:.4f}\")\n",
                "\n",
                "# --- 3. PLOT THE CONVERGENCE CURVE ---\n",
                "\n",
                "# Convert the results to a Pandas DataFrame for plotting\n",
                "df_errors = pd.DataFrame(error_data, columns=['Iterations', 'RMSE'])\n",
                "\n",
                "# Plotting the curve\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(df_errors['Iterations'], df_errors['RMSE'], marker='o', linestyle='-', color='b')\n",
                "plt.title('ALS Model Convergence (Test Set RMSE vs. Iterations)')\n",
                "plt.xlabel('Training Steps (maxIter)')\n",
                "plt.ylabel('Test Set RMSE')\n",
                "plt.grid(True)\n",
                "plt.savefig(\"als_convergence_plot.png\")\n",
                "plt.close()\n",
                "\n",
                "print(\"\\nConvergence plot saved to als_convergence_plot.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# cleanup spark instance\n",
                "spark.stop()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "outfit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
